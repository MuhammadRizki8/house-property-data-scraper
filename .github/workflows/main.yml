name: Bandung Property Scraper

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      mode:
        description: 'Scraping mode'
        required: true
        default: 'links'
        type: choice
        options:
          - links
          - details
          - both
      start_page:
        description: 'Starting page number'
        required: false
        default: '1'
        type: string
      pages:
        description: 'Number of pages to scrape'
        required: false
        default: '5'
        type: string
      delay_min:
        description: 'Minimum delay between requests (seconds)'
        required: false
        default: '2'
        type: string
      delay_max:
        description: 'Maximum delay between requests (seconds)'
        required: false
        default: '4'
        type: string
      url:
        description: 'Base URL for property scraping'
        required: false
        default: 'https://www.rumah123.com/jual/bandung/rumah/'
        type: string
      links_file:
        description: 'Path to links file (for details mode only)'
        required: false
        default: ''
        type: string
      start_link:
        description: 'Starting link number (for details mode only)'
        required: false
        default: '1'
        type: string

  # Schedule to run daily at 2 AM UTC (9 AM WIB)
  schedule:
    - cron: '0 2 * * *'

  # Trigger on push to main
  push:
    branches:
      - main

jobs:
  scrape-bandung-property:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create results directory
        run: mkdir -p results

      - name: Run property scraper
        run: |
          # Build command dynamically based on inputs
          CMD="python main.py"
          CMD="$CMD --mode ${{ github.event.inputs.mode || 'links' }}"

          # Add start-page if provided
          if [ -n "${{ github.event.inputs.start_page }}" ] && [ "${{ github.event.inputs.start_page }}" != "1" ]; then
            CMD="$CMD --start-page ${{ github.event.inputs.start_page }}"
          fi

          # Add pages for links or both mode
          if [ "${{ github.event.inputs.mode || 'links' }}" = "links" ] || [ "${{ github.event.inputs.mode || 'links' }}" = "both" ]; then
            CMD="$CMD --pages ${{ github.event.inputs.pages || '5' }}"
            CMD="$CMD --url '${{ github.event.inputs.url || 'https://www.rumah123.com/jual/bandung/rumah/' }}'"
          fi

          # Add links-file and start-link for details mode
          if [ "${{ github.event.inputs.mode || 'links' }}" = "details" ]; then
            if [ -n "${{ github.event.inputs.links_file }}" ]; then
              CMD="$CMD --links-file '${{ github.event.inputs.links_file }}'"
            fi
            if [ -n "${{ github.event.inputs.start_link }}" ] && [ "${{ github.event.inputs.start_link }}" != "1" ]; then
              CMD="$CMD --start-link ${{ github.event.inputs.start_link }}"
            fi
          fi

          # Add delay parameters
          CMD="$CMD --delay-min ${{ github.event.inputs.delay_min || '2' }}"
          CMD="$CMD --delay-max ${{ github.event.inputs.delay_max || '4' }}"

          echo "Running command: $CMD"
          eval $CMD

      - name: Get timestamp for artifact naming
        id: timestamp
        run: echo "timestamp=$(date +'%Y%m%d_%H%M%S')" >> $GITHUB_OUTPUT

      - name: Upload scraped links as artifact
        uses: actions/upload-artifact@v4
        with:
          name: bandung-property-scraper-${{ github.event.inputs.mode || 'links' }}-${{ steps.timestamp.outputs.timestamp }}
          path: |
            results/
            *.log
          retention-days: 30

      - name: Display scraping summary
        run: |
          echo "=== Scraping Summary ==="
          echo "Mode: ${{ github.event.inputs.mode || 'links' }}"
          echo "Start Page: ${{ github.event.inputs.start_page || '1' }}"
          echo "Pages: ${{ github.event.inputs.pages || '5' }}"
          echo "URL: ${{ github.event.inputs.url || 'https://www.rumah123.com/jual/bandung/rumah/' }}"

          if [ -d "results" ]; then
            echo ""
            echo "Results directory contents:"
            ls -la results/
            
            # Find the latest scraping session directory
            LATEST_DIR=$(find results -name "scraping_session_*" -type d | sort | tail -1)
            if [ -n "$LATEST_DIR" ]; then
              echo ""
              echo "Latest session: $LATEST_DIR"
              echo "Files in latest session:"
              ls -la "$LATEST_DIR"
              
              # Count links if file exists
              if [ -f "$LATEST_DIR/property_links.txt" ]; then
                LINK_COUNT=$(wc -l < "$LATEST_DIR/property_links.txt")
                echo ""
                echo "Total property links scraped: $LINK_COUNT"
              fi
              
              # Count properties if CSV exists
              if [ -f "$LATEST_DIR/rumah123_properties_final.csv" ]; then
                PROP_COUNT=$(tail -n +2 "$LATEST_DIR/rumah123_properties_final.csv" | wc -l)
                echo "Total property details scraped: $PROP_COUNT"
              fi
            fi
          fi

          # Show last few lines of log if exists
          if [ -f "scraping.log" ]; then
            echo ""
            echo "=== Last 10 lines of scraping log ==="
            tail -10 scraping.log
          fi
